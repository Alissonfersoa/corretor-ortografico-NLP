{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a09ecbf",
   "metadata": {},
   "source": [
    "# Corretor Ortográfico (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3aea76",
   "metadata": {},
   "source": [
    "Desenvolvimento de um corretor ortográfico utilizando Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5861d",
   "metadata": {},
   "source": [
    "# 1. - Importar arquivo e leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7687d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "imagem \n",
      "\n",
      "Temos a seguinte classe que representa um usuário no nosso sistema:\n",
      "\n",
      "java\n",
      "\n",
      "Para salvar um novo usuário, várias validações são feitas, como por exemplo: Ver se o nome só contém letras, [**o CPF só números**] e ver se o usuário possui no mínimo 18 anos. Veja o método que faz essa validação:\n",
      "\n",
      "java \n",
      "\n",
      "Suponha agora que eu tenha outra classe, a classe `Produto`, que contém um atributo nome e eu quero fazer a mesma validação que fiz para o nome do usuário: Ver se só contém letras. E aí? Vou\n"
     ]
    }
   ],
   "source": [
    "# Abrindo o arquivo txt, armazenar seu conteúdo em uma variável declarada.\n",
    "\n",
    "with open('C:\\\\Users\\\\batis\\\\Documents\\\\Jupyter_Notebook\\\\treinamento.txt', encoding=\"utf8\",mode='r') as f:\n",
    "    treinamento = f.read()\n",
    "    \n",
    "# Lendo arquivo e imprimindo o conteúdo\n",
    "print(treinamento[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c828ba",
   "metadata": {},
   "source": [
    "Depois de fazer a leitura do arquivo temos que fazer a \"tokenização\" que é a separação do texto em \"tokens\".\n",
    "\n",
    "**Definição de Token:**\n",
    "    - Token é uma sequência de caracteres, separados por um limitador, que pode ser um espaço em branco, pontuação ou quebra de linhas, por exemplo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "566c9183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2605046"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrando quantos caracteres possui o arquivo\n",
    "len(treinamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc415a",
   "metadata": {},
   "source": [
    "# 2. - Fatiando o conteúdo do arquivo\n",
    "\n",
    "Utilizando o método _**split()**_ podemos identificar quantas palavras compoem o arquivo e não a quantidade de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ccab3b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Olá', 'Mundo!!', 'Terra', 'falando.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo da utilização do método\n",
    "exemplo_texto = 'Olá Mundo!! Terra falando.'\n",
    "\n",
    "# Tokenizando o texto com o split()\n",
    "tokens = exemplo_texto.split()\n",
    "\n",
    "# Imprimindo o resultado, ele separa as palavras pelo 'espaço' entre elas\n",
    "print(tokens)\n",
    "\n",
    "# Contando o tamanho da lista gerada\n",
    "len(tokens) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38963281",
   "metadata": {},
   "source": [
    "**Import lib nltk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ccbb20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\batis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import lib e baixando o pacote punkt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "617496f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Olá', 'Mundo', '!', '!', 'Terra', 'falando', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizando o texto utilizando a lib nltk e mostrando o resultado\n",
    "\n",
    "palavras_separadas = nltk.tokenize.word_tokenize(exemplo_texto)\n",
    "print(palavras_separadas)\n",
    "len(palavras_separadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc4d15e",
   "metadata": {},
   "source": [
    "### Nota/Observação\n",
    "\n",
    "Notamos que ao utilizar a biblioteca nltk, é possível separar os caracteres (seja caractere especial ou pontuação) das palavras encontradas, porém ainda fazem parte de uma mesma lista."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b54a1",
   "metadata": {},
   "source": [
    "# 2.1 - Separando os dados\n",
    "\n",
    "Para realizar a separação utilizo o método isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b02921c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/.'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "06356ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ç'.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6e2d9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'à'.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7007f06",
   "metadata": {},
   "source": [
    "### Nota/Observação\n",
    "Este método consiste em analisar e verificar se em uma sequência existe apenas letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d95795b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Olá', 'Mundo', 'Terra', 'falando']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando uma função para separar as palavras\n",
    "def separa_palavras(lista_tokens):\n",
    "    \n",
    "    # Criando uma lista vázia\n",
    "    lista_palavras = []\n",
    "    \n",
    "    for token in lista_tokens:\n",
    "        \n",
    "        if token.isalpha():\n",
    "            \n",
    "            lista_palavras.append(token)\n",
    "    \n",
    "    return lista_palavras\n",
    "\n",
    "# Chamando a função e passando o parâmetro\n",
    "separa_palavras(palavras_separadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db362c47",
   "metadata": {},
   "source": [
    "Função criada, vamos para o próximo passo, criar o nosso corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bbf6f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número total de palavras em nosso corpus é de : 403104\n"
     ]
    }
   ],
   "source": [
    "lista_tokens = nltk.tokenize.word_tokenize(treinamento)\n",
    "\n",
    "# Chamando a função e passando o corpus como parâmetro\n",
    "lista_palavras = separa_palavras(lista_tokens)\n",
    "\n",
    "# Imprimindo o resultado\n",
    "print(f'O número total de palavras em nosso corpus é de : {len(lista_palavras)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0839f464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imagem', 'Temos', 'a', 'seguinte', 'classe']\n"
     ]
    }
   ],
   "source": [
    "# Amostragem dos resultados\n",
    "print(lista_palavras[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb366d",
   "metadata": {},
   "source": [
    "# 3. - Normalização\n",
    "\n",
    "Normalizar as palavras é passar todas para um mesmo padrão, como por exemplo deixar todas em letras minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fe975b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imagem', 'temos', 'a', 'seguinte', 'classe', 'que', 'representa', 'um', 'usuário', 'no']\n"
     ]
    }
   ],
   "source": [
    "# Criando uma função de normalização utilizando o método lower()\n",
    "def normalizacao(lista_palavras):\n",
    "    \n",
    "    lista_normalizada = []\n",
    "    \n",
    "    for palavra in lista_palavras:\n",
    "        \n",
    "        lista_normalizada.append(palavra.lower())\n",
    "        \n",
    "    return lista_normalizada\n",
    "\n",
    "# Chamando a função e passando os parâmetros\n",
    "lista_normalizada = normalizacao(lista_palavras)\n",
    "\n",
    "# Imprimindo uma amostra do resultado\n",
    "print(lista_normalizada[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bead0d0a",
   "metadata": {},
   "source": [
    "### Nota/Observação\n",
    "Agora temos todas as palavras em letras minúsculas, facilitando a normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f30f9b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número total de palavras em nosso corpus é de: 18465\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o total real de palavras existentes em nosso corpus\n",
    "print(f'O número total de palavras em nosso corpus é de: {len(set(lista_normalizada))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b055e",
   "metadata": {},
   "source": [
    "# 4. - Preparando estrutura do corretor\n",
    "Neste passo precisamos ensinar o nosso corretor a identificar erros e desvios de digitação para depois corrigí-los.\n",
    "\n",
    "A ideia é criar funções capazes de tratar cada tipo de erro identificados no conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e938ccc",
   "metadata": {},
   "source": [
    "# 4.1 - Tratativa do erro: Falta uma letra\n",
    "\n",
    "Para este caso vamos criar um exemplo, supondo que ao digitar a palavra **\"ortografia\"** sem querer enguli uma letra e saiu: **\"otografia\"**.\n",
    "\n",
    "Primeiro passo para corrigir e fatiar em duas partes a palavra no lugar exato e inserir a letra faltante entre a lacuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "54a17bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('o', 'tografia')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo\n",
    "\n",
    "exemplo_palavra = 'otografia'\n",
    "\n",
    "# Método slicing para fatiar uma string\n",
    "(exemplo_palavra[:1], exemplo_palavra[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8a99db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o + \"a letra que falta (R)\" + tografia\n",
      "\n",
      "Resulta na palavra \n",
      "\n",
      "ortografia\n"
     ]
    }
   ],
   "source": [
    "# Exemplo da função para corrigir\n",
    "print(f'{exemplo_palavra[:1]} + \"a letra que falta (R)\" + {exemplo_palavra[1:]}'\n",
    "    f'\\n\\nResulta na palavra \\n\\n{exemplo_palavra[:1] + \"r\" + exemplo_palavra[1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40818f52",
   "metadata": {},
   "source": [
    "Próximo passo é fazer isso para todas as palavras que estiverem faltando uma letra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "291aa588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oatografia',\n",
       " 'obtografia',\n",
       " 'octografia',\n",
       " 'oetografia',\n",
       " 'odtografia',\n",
       " 'oftografia',\n",
       " 'ogtografia',\n",
       " 'ohtografia',\n",
       " 'oitografia',\n",
       " 'ojtografia']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando função para inserir as letras\n",
    "\n",
    "def insere_letras(fatias):\n",
    "    \n",
    "    novas_palavras =[]\n",
    "    \n",
    "    letras = 'abcedfghijklmnopqrstuvwxyzáâàãéêèíîìóôòõúûùç'\n",
    "    \n",
    "    for esquerdo, direito in fatias:\n",
    "        \n",
    "        for letra in letras:\n",
    "            \n",
    "            novas_palavras.append(esquerdo + letra + direito)\n",
    "    \n",
    "    return novas_palavras\n",
    "\n",
    "# Chamando a função e passando os parâmetros\n",
    "# Imprimindo uma amostra do resultado\n",
    "insere_letras([('o', 'tografia')])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ca5457d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aotografia', 'botografia', 'cotografia', 'eotografia', 'dotografia', 'fotografia', 'gotografia', 'hotografia', 'iotografia', 'jotografia']\n"
     ]
    }
   ],
   "source": [
    "# Função para gerar as palavras\n",
    "def gerador_palavras(palavra):\n",
    "    \n",
    "    fatias = []\n",
    "    \n",
    "    for i in range(len(palavra) + 1):\n",
    "        \n",
    "        fatias.append((palavra[:i], palavra[i:]))\n",
    "    \n",
    "    palavras_geradas = insere_letras(fatias)\n",
    "    \n",
    "    return palavras_geradas\n",
    "\n",
    "palavras_geradas = gerador_palavras(exemplo_palavra)\n",
    "\n",
    "# Imprimindo a palavra de exemplo com uma amostra do resultado\n",
    "print(palavras_geradas[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "285ebd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A palavra correta é \"ortografia\" e está dentro da lista gerada!\n"
     ]
    }
   ],
   "source": [
    "# Verifica se a palavra correta está dentro da lista gerada\n",
    "for palavra in palavras_geradas:\n",
    "    \n",
    "    if palavra == 'ortografia':\n",
    "        \n",
    "        print(f'A palavra correta é \"{palavra}\" e está dentro da lista gerada!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721eee8",
   "metadata": {},
   "source": [
    "Após a verificação o corretor precisa saber qual a palavra correta dentro da lista de palavras que foi gerado pela função\n",
    "\n",
    "Antes de seguir podemos ver a quantidade de palavras geradas anteriormente pela função com o método **len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bc9182b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram geradas 440 palavras\n"
     ]
    }
   ],
   "source": [
    "print(f'Foram geradas {len(palavras_geradas)} palavras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b1580",
   "metadata": {},
   "source": [
    "Para está etapa do corretor podemos usar a função **FreqDist()** da lib nltk.\n",
    "\n",
    "Esse método calcula a frequência que uma determinada palavra em um conjunto dentro do corpus.\n",
    "\n",
    "Utilizando esse método é preciso passar como input a palavra que desejamos saber a frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bc831e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 15502),\n",
       " ('o', 14056),\n",
       " ('que', 12230),\n",
       " ('a', 11099),\n",
       " ('e', 10501),\n",
       " ('para', 7710),\n",
       " ('um', 6368),\n",
       " ('é', 5899),\n",
       " ('uma', 5220),\n",
       " ('do', 5124)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chamando a função FreqDist\n",
    "# Passando como parânmetro de input a lista normalizada \n",
    "frequencia = nltk.FreqDist(lista_normalizada)\n",
    "\n",
    "total_palavras = len(lista_normalizada)\n",
    "\n",
    "# Imprimindo as 10 palavras mais comuns dentro da lista\n",
    "frequencia.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9392da",
   "metadata": {},
   "source": [
    "### Nota/Observação\n",
    "Com este dado, podemos agora calcular a probabilidade de uma determinada palavra aparecer em nosso corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f8e5e6",
   "metadata": {},
   "source": [
    "Para isso, vamos criar uma função para cálculo da probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c697bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função para probabilidade\n",
    "def probabilidade(palavra_gerada):\n",
    "    return frequencia[palavra_gerada] / total_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e207f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando função corretor\n",
    "def corretor(palavra_errada):\n",
    "    \n",
    "    palavra_geradas = gerador_palavras(palavra_errada)\n",
    "    \n",
    "    palavra_correta = max(palavras_geradas, key=probabilidade)\n",
    "    \n",
    "    # Retorna a palavra já corrigida\n",
    "    return palavra_correta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3fd705",
   "metadata": {},
   "source": [
    "Feito a criação das funções iniciais, vamos recriar o exemplo passando a palavra para corrigir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18eb5ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fotografia'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Palavra incorreta\n",
    "exemplo_palavra = 'orografia'\n",
    "\n",
    "# Chamando a função do corretor\n",
    "corretor(exemplo_palavra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de87ea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você quis dizer: fotografia\n"
     ]
    }
   ],
   "source": [
    "# Teste para a palavra lógica\n",
    "\n",
    "teste = 'lgica'\n",
    "print(f'Você quis dizer: {corretor(teste)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8f5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20129f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddc5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15deb30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd752b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c356252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfaffbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7cabb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef01aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9417b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe7831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e8db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d1424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475b6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65534b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb309e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6fcc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebac9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564eb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601306ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaff2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
